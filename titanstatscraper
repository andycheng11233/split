import argparse
import asyncio
import json
import logging
import re
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from playwright.async_api import async_playwright
import openpyxl
from bs4 import BeautifulSoup

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger("titan_stat_main")

INDEX_URL = "https://live.titan007.com/indexall_big.aspx"
GAME_FILE = "game.json"

BAD_STRING = "暫無數據"
MIN_SECTIONS_FOR_FULL = 3  # Amount required to call match 'full'--adjust as needed.

SECTIONS_TO_TRY = [
    ("league_standings",           r"聯賽積分排名"),
    ("head_to_head",               r"對賽往績"),
    ("data_comparison",            r"數據對比"),
    ("referee_stats",              r"裁判統計"),
    ("league_trend",               r"聯賽盤路走勢"),
    ("same_trend",                 r"相同盤路"),
    ("goal_distribution",          r"入球數/上下半場入球分布"),
    ("halftime_fulltime",          r"半全場"),
    ("goal_count",                 r"進球數/單雙"),
    ("goal_time",                  r"進球時間"),
    ("future_matches",             r"未來五場"),
    ("pre_match_brief",            r"賽前簡報"),
    ("season_stats_comparison",    r"本賽季數據統計比較"),
    ("recent_form",                r"近期戰績"),
    ("last_match_player_ratings",  r"球員上一場出場評分"),
    ("lineup_and_injuries",        r"陣容情況"),
    ("pre_match_table",            r"賽前積分榜"),
]

def _norm(val: Any) -> str:
    s = str(val).strip() if val is not None else ""
    return "" if s.lower() in {"", "null", "none", "undefined"} else s

def load_gamejson():
    with open(GAME_FILE, encoding="utf-8") as f:
        ls = json.load(f)
    gids, subids, simp_trad = set(), set(), set()
    for x in ls:
        sclass = _norm(x.get("SclassID"))
        sub = _norm(x.get("SubID"))
        simp = _norm(x.get("simp"))
        trad = _norm(x.get("trad"))
        if sclass:
            gids.add(sclass)
        if sub:
            subids.add(sub)
        if simp:
            simp_trad.add(simp)
        if trad:
            simp_trad.add(trad)
    return gids, subids, simp_trad

async def filter_match_ids_only_my_leagues(all_match_ids: List[str]) -> Tuple[List[str], Dict[str, str]]:
    """
    Returns (filtered_ids, league_lookup) where league_lookup[mid] = league name (if found).
    """
    gids, subids, names = load_gamejson()
    filtered: List[str] = []
    league_lookup: Dict[str, str] = {}

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        await page.goto(INDEX_URL)
        await page.wait_for_timeout(2_200)

        mapping = await page.evaluate("""
            () => {
              let out = {};
              let sources = [window.A, window.arr, window.matchList, window.B, window.C, window.Match];
              for (const src of sources) {
                if (Array.isArray(src)) {
                  for (const row of src) {
                    if (!row) continue;
                    let mid = String(row[0] || row.matchid || row.MatchID || row.matchId || row.id || "");
                    let sclassid = String(row[2] || row[4] || row.sclassid || row.SclassID || row.sclassID || "");
                    let subid = String(row[9] || row.subid || row.SubID || "");
                    let league = row[1] || row.league || "";
                    let simp = row[1] || row.simp || "";
                    let trad = row[1] || row.trad || "";
                    out[mid] = { sclassid, subid, simp, trad, league };
                  }
                }
              }
              return out;
            }
        """)

        for mid in all_match_ids:
            if not mid.isdigit():
                continue
            info = mapping.get(mid) or {}
            scid = _norm(info.get("sclassid"))
            subid = _norm(info.get("subid"))
            simp = _norm(info.get("simp"))
            trad = _norm(info.get("trad"))
            league = _norm(info.get("league"))
            if league:
                league_lookup[mid] = league

            if (scid and scid in gids) or (subid and subid in subids) or (simp and simp in names) or (trad and trad in names):
                filtered.append(mid)
                continue

            tr = await page.query_selector(f'tr[id="tr1_{mid}"]')
            if tr:
                tds = await tr.query_selector_all("td")
                league_name = _norm((await tds[1].inner_text()) if len(tds) > 1 else "")
                if league_name:
                    league_lookup[mid] = league_name
                if league_name and league_name in names:
                    filtered.append(mid)

        await browser.close()
    return filtered, league_lookup

def is_real_data_table(tbl):
    rows = tbl.find_all("tr")
    if len(rows) < 2:
        return False
    for row in rows:
        cells = [c.get_text(strip=True) for c in row.find_all(['td', 'th'])]
        for cell in cells:
            if cell and BAD_STRING not in cell:
                return True
    return False

def extract_table_text(tbl):
    """Return table as list of rows (each row is list of cell strings)."""
    return [
        [cell.get_text(strip=True) for cell in row.find_all(['th', 'td'])]
        for row in tbl.find_all("tr")
    ]

def analyze_sections(html):
    soup = BeautifulSoup(html, "html.parser")
    found, missing = [], []
    debug_info = {}
    tables: Dict[str, List[List[str]]] = {}

    for key, regex in SECTIONS_TO_TRY:
        header = soup.find(string=re.compile(regex))
        section_status = "not_found"
        if header:
            tbl = header.find_next("table")
            if tbl and is_real_data_table(tbl):
                section_status = "real_data"
                found.append(key)
                tables[key] = extract_table_text(tbl)
            elif tbl:
                section_status = "only_bad_string"
                missing.append(key)
            else:
                section_status = "table_not_found"
                missing.append(key)
        else:
            missing.append(key)
        debug_info[key] = section_status
    return found, missing, debug_info, tables

async def titan_scrape_stats(match_id: str):
    url = f"https://zq.titan007.com/analysis/{match_id}.htm"
    out: Dict[str, Any] = {
        "match_id": match_id,
        "url": url,
        "scraped_at": datetime.now().isoformat(),
        "sections_found": [],
        "sections_missing": [],
        "sections_debug": {},
        "sections_data": {},  # actual tables
        "has_stats": False,
        "home_team": "",
        "away_team": "",
        "game_time": "",
        "league": "",
    }
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        try:
            await page.goto(url, wait_until="networkidle", timeout=60000)
            await asyncio.sleep(2.5)
            html = await page.content()
        except Exception as e:
            out["error"] = str(e)
            await browser.close()
            return out
        await browser.close()

    found, missing, debug, tables = analyze_sections(html)
    out["sections_found"] = found
    out["sections_missing"] = missing
    out["sections_debug"] = debug
    out["sections_data"] = tables
    out["has_stats"] = len(found) >= MIN_SECTIONS_FOR_FULL

    soup = BeautifulSoup(html, "html.parser")
    title_text = ""
    if soup.title:
        try:
            title_text = soup.title.get_text(strip=True)
        except Exception:
            title_text = ""
    if title_text:
        m = re.search(r"(.+?) vs (.+?)[\s|【]", title_text)
        if m:
            out["home_team"] = m.group(1).strip()
            out["away_team"] = m.group(2).strip()

    return out

def write_report_excel(excel_path: Path, full_rows: List[List[str]], missing_rows: List[List[str]]):
    headers = ["match_id", "home_team", "away_team", "game_time", "league", "stat_url", "found_sections", "missing_sections"]
    wb = openpyxl.Workbook()
    ws_full = wb.active
    ws_full.title = "captured"
    ws_full.append(headers)
    for row in full_rows:
        ws_full.append(row)

    ws_miss = wb.create_sheet("missing")
    ws_miss.append(headers)
    for row in missing_rows:
        ws_miss.append(row)

    wb.save(excel_path)

async def discover_match_ids(limit: Optional[int], min_id: int) -> List[str]:
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        await page.goto(INDEX_URL, wait_until="domcontentloaded", timeout=60000)
        await asyncio.sleep(4.0)
        ids = await page.evaluate(
            """
            () => {
              const out = new Set();
              const globals = ['A','B','C','arr','arrData','Match','matchList'];
              for (const k of globals) {
                const v = window[k];
                if (Array.isArray(v)) {
                  for (const row of v) {
                    const cand = Array.isArray(row)
                      ? row[0]
                      : (row && (row.matchid || row.MatchID || row.matchId || row.id));
                    if (cand) out.add(String(cand));
                  }
                }
              }
              document.querySelectorAll('a[href*="analysis/"][href$=".htm"]').forEach(a => {
                const m = a.href.match(/analysis\\/(\\d+)\\.htm/);
                if (m) out.add(m[1]);
              });
              return Array.from(out);
            }
            """
        )
        await browser.close()

    uniq = []
    seen = set()
    for mid in ids:
        if not (mid and mid.isdigit()):
            continue
        if int(mid) < min_id:
            continue
        if mid not in seen:
            seen.add(mid)
            uniq.append(mid)

    if limit:
        uniq = uniq[:limit]
    logger.info("Discovered %d match ids from live page (limit=%s, min_id=%s)", len(uniq), limit, min_id)
    return uniq

async def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--limit", type=int, help="Limit how many IDs from discovery")
    parser.add_argument("--base", default="titan/stats", help="Base stats directory (flat layout)")
    parser.add_argument("--min-id", type=int, default=1_000_000, help="Ignore match IDs below this value (filters out ancient IDs)")
    args = parser.parse_args()

    base = Path(args.base)
    full_dir = base / "full"
    missing_dir = base / "missing"
    full_dir.mkdir(parents=True, exist_ok=True)
    missing_dir.mkdir(parents=True, exist_ok=True)

    ids = await discover_match_ids(args.limit, args.min_id)
    ids, league_lookup = await filter_match_ids_only_my_leagues(ids)
    logger.info("Filtered: %d match ids will be scraped.", len(ids))

    leagues_to_scrape = sorted({league_lookup[mid] for mid in ids if league_lookup.get(mid)})

    full_rows = []
    missing_rows = []
    cur_time_tag = datetime.now().strftime("%Y%m%d%H%M")
    excel_path = Path(f"report{cur_time_tag}.xlsx")

    total = len(ids)
    print(f"Leagues to scrape ({len(leagues_to_scrape)}): {leagues_to_scrape}")
    print(f"Ready to process {total} matches:\n{ids}")

    for idx, mid in enumerate(ids, start=1):
        league_name = league_lookup.get(mid, "")
        print(f"[{idx}/{total}] Scraping {mid} | League: {league_name or 'unknown'} ...")
        out = await titan_scrape_stats(mid)
        if league_name:
            out["league"] = league_name
        row = [
            out["match_id"],
            out.get("home_team", ""),
            out.get("away_team", ""),
            out.get("game_time", ""),
            league_name or out.get("league", ""),
            out.get("url"),
            ";".join(out.get("sections_found") or []),
            ";".join(out.get("sections_missing") or [])
        ]
        if out.get("has_stats"):
            (full_dir / f"{mid}.json").write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding="utf-8")
            full_rows.append(row)
            print(f"[{idx}/{total}] OK - FULL: {mid} | League: {league_name or 'unknown'}")
        else:
            (missing_dir / f"{mid}.json").write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding="utf-8")
            missing_rows.append(row)
            print(f"[{idx}/{total}] WARNING - MISSING: {mid} | League: {league_name or 'unknown'} | Sections found: {out['sections_found']}")

    write_report_excel(excel_path, full_rows, missing_rows)
    logger.info(f"Excel written: {excel_path} (captured={len(full_rows)}, missing={len(missing_rows)})")
    print(f"Completed scrape. {len(missing_rows)} matches missing stats. See Excel/log as configured. Report: {excel_path}")

if __name__ == "__main__":
    asyncio.run(main())
